# -*- coding: utf-8 -*-
"""lenny_cuml_test-cut-down-and-fast-bak.ipynb

Automatically generated by Colab.

"""

from google.colab import drive

drive.mount("/content/drive")

import locale

locale.getpreferredencoding = lambda: "UTF-8"

#!pip install -r /content/drive/MyDrive/NASA/lenny_google_collab/simple_requirements.txt

#!pip install rasterio
#!pip install scikit-learn==1.3.0

import joblib
import rasterio
import matplotlib.pyplot as plt
import time
import numpy as np
import pandas as pd

input_tif = "/content/drive/MyDrive/NASA/lenny_google_collab/S2_Chautauqua_v2.tif"
with rasterio.open(input_tif) as src:
    raster_data = src.read()
    profile = src.profile  # Get the profile of the existing raster

# define constants for new bands
SA_constant = 43343
Max_depth_constant = 618
pct_dev_constant = 7.609999999999999
pct_ag_constant = 48.980000000000004

# create new bands
SA_band = np.full_like(raster_data[0], SA_constant, dtype=raster_data.dtype)
Max_depth_band = np.full_like(
    raster_data[0], Max_depth_constant, dtype=raster_data.dtype
)
pct_dev_band = np.full_like(raster_data[0], pct_dev_constant, dtype=raster_data.dtype)
pct_ag_band = np.full_like(raster_data[0], pct_ag_constant, dtype=raster_data.dtype)

# update profile to reflect additional bands
profile.update(count=9)  # (update count to include original bands + 4 new bands)

# output GeoTIFF file
output_tif = "S2_Chautauqua_v2_modified.tif"

# write the modified raster data to the new GeoTIFF file
with rasterio.open(output_tif, "w", **profile) as dst:
    # write original bands
    for i in range(1, raster_data.shape[0] + 1):
        dst.write(raster_data[i - 1], indexes=i)

    # write additional bands
    dst.write(SA_band, indexes=raster_data.shape[0] + 1)
    dst.write(Max_depth_band, indexes=raster_data.shape[0] + 2)
    dst.write(pct_dev_band, indexes=raster_data.shape[0] + 3)
    dst.write(pct_ag_band, indexes=raster_data.shape[0] + 4)

    dst.transform = src.transform
    dst.crs = src.crs

print("Created a modified TIF with the four extra bands data from constants")

#!git clone https://github.com/rapidsai/rapidsai-csp-utils.git
#!python rapidsai-csp-utils/colab/pip-install.py

#!pip install dask-ml

#!pip freeze

from dask.distributed import Client
from dask_cuda import LocalCUDACluster

cluster = LocalCUDACluster()
client = Client(cluster)

client

import cudf
import pandas as pd


def prepare_data(df_path, lagosid_path, lulc_path, random_state=621, test_size=0.1):
    # read csvs
    df = pd.read_csv(df_path)

    lagosid = pd.read_csv(lagosid_path)
    print("CSV imported")

    # select relevant columns from lagosid
    lagosid = lagosid[["lagoslakei"]]
    df = pd.concat([lagosid, df], axis=1)

    # filter to Sentinel-2
    # df = df.loc[df['satellite'].isin(["1","2"])]

    # filter to LC
    # df = df.loc[df['satellite'].isin(["LC08","LC09"])]
    df = df[df["443"].notna()]

    # filter to low chl-a
    # df = df[df['chl_a'] < 15]

    # filter to strict date range
    # df = df[df['date_diff'] < 4]

    # create chl-a mathematical inputs
    df["NDCI"] = (df["703"] - df["665"]) / (df["703"] + df["665"])
    df["NDVI"] = (df["864"] - df["665"]) / (df["864"] + df["665"])
    df["3BDA"] = ((df["493"] - df["665"]) / (df["493"] + df["665"])) - df["560"]

    # create rough vol based on depth & SA
    df["lake_vol"] = df["SA"] * df["Max.depth"]

    # load lagos-ne iws data, merged with dataframe
    iws_lulc = pd.read_csv(lulc_path)

    # summarize percent developed land
    iws_lulc["iws_nlcd2011_pct_dev"] = (
        iws_lulc["iws_nlcd2011_pct_21"]
        + iws_lulc["iws_nlcd2011_pct_22"]
        + iws_lulc["iws_nlcd2011_pct_23"]
        + iws_lulc["iws_nlcd2011_pct_24"]
    )

    # summarize percent agricultural land
    iws_lulc["iws_nlcd2011_pct_ag"] = (
        iws_lulc["iws_nlcd2011_pct_81"] + iws_lulc["iws_nlcd2011_pct_82"]
    )

    # create new dataframe with the two variables
    iws_human = iws_lulc[["lagoslakeid", "iws_nlcd2011_pct_dev", "iws_nlcd2011_pct_ag"]]

    iws_human = iws_human.rename(
        columns={
            "lagoslakeid": "lagoslakei",
            "iws_nlcd2011_pct_dev": "pct_dev",
            "iws_nlcd2011_pct_ag": "pct_ag",
        }
    )

    # left join df with iws_human
    df = df.merge(iws_human, on="lagoslakei")

    # set dfs
    # filter nas for morphology
    df = df[df["SA"].notna()]
    df = df[df["Max.depth"].notna()]

    # filter nas if using landuse
    df = df[df["pct_dev"].notna()]
    df = df[df["pct_ag"].notna()]

    # print(df.head)
    features = df[
        ["443", "493", "560", "665", "864", "SA", "Max.depth", "pct_dev", "pct_ag"]
    ]
    feature_names = [
        "443",
        "493",
        "560",
        "665",
        "864",
        "SA",
        "Max.depth",
        "pct_dev",
        "pct_ag",
    ]

    features = features.to_numpy()
    chla = df[["chl_a"]].to_numpy()
    weights = df[["weight"]].to_numpy()
    lat = df[["MEAN_lat"]].to_numpy()
    lon = df[["MEAN_long"]].to_numpy()
    coordinates = (df.MEAN_lat, df.MEAN_long)
    """
    # squeeze converts dataframe to series
    lagosid = df[["lagoslakei"]].squeeze()

    # initialize random effects covariate
    Z = np.full((lagosid.size, 1), 1)

    # initial train-test split (holdout 10% for final training)
    x_train, x_test, y_train, y_test, wt_train, wt_test, lon_train, lon_test, lat_train, lat_test, clusters_train, clusters_test, z_train, z_test = train_test_split(
        features, chla, weights, lon, lat, lagosid, Z, test_size=test_size, random_state=random_state)

    return x_train, x_test, y_train, y_test, wt_train, wt_test, lon_train, lon_test, lat_train, lat_test, clusters_train, clusters_test, z_train, z_test
    """
    df = df[
        [
            "chl_a",
            "443",
            "493",
            "560",
            "665",
            "864",
            "SA",
            "Max.depth",
            "pct_dev",
            "pct_ag",
        ]
    ]
    input_cols = [
        "443",
        "493",
        "560",
        "665",
        "864",
        "SA",
        "Max.depth",
        "pct_dev",
        "pct_ag",
    ]
    for col in df.select_dtypes(["object"]).columns:
        df[col] = df[col].astype("category").cat.codes.astype(np.int32)

    # cast all columns to int32
    for col in df.columns:
        df[col] = df[col].astype(np.float32)  # needed for random forest

    # put target/label column first [ classic XGBoost standard ]
    output_cols = ["chl_a"] + input_cols
    df.to_csv("preindexaspandas.csv")
    df = df.reindex(columns=output_cols)

    print(df.shape)
    print(df["chl_a"].shape)
    df.to_csv("postindexaspandas.csv")
    df_cudf = cudf.from_pandas(df)
    return df_cudf


import pandas as pd
from cuml.ensemble import RandomForestRegressor
from dask_ml.model_selection import RandomizedSearchCV
from cuml.model_selection import train_test_split
from sklearn.metrics import r2_score
import numpy as np
import cudf
import time

# Load the CSV file OR create it
# Creating the proper dataframe (csv) from other datasets
df_path = "/content/drive/MyDrive/NASA/lenny_google_collab/ccri_tidy_chla_processed_data_V2.csv"
lagosid_path = (
    "/content/drive/MyDrive/NASA/lenny_google_collab/ccri_lakes_withLagosID.csv"
)
lulc_path = "/content/drive/MyDrive/NASA/lenny_google_collab/LAGOSNE_iws_lulc105.csv"
data = prepare_data(df_path, lagosid_path, lulc_path)

# Loading the csv
# file_path = '/content/drive/MyDrive/NASA/lenny_google_collab/postindexaspandas.csv'
# data = cudf.read_csv(file_path)
# data = data.drop(columns=['Unnamed: 0'])


data = data[
    data["chl_a"] < 350
]  # most values are 0-100, remove the crazy 4,000 outlier
# print(data.head)

# Convert float columns to integers
for col in data.columns:
    data[col] = data[col].astype(np.float32)

# print(data.head)


X = data.drop(columns=["chl_a"])
y = data["chl_a"]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]
# Number of features to consider at every split
max_features = [
    "log2",
    "sqrt",
    1.0,
]  # all in the following array are fast but log2 is fastest ['log2', 'sqrt', 1.0]
# Maximum number of levels in tree
max_depth = [int(x) for x in np.linspace(10, 110, num=11)]
# max_depth.append(None)
# Minimum number of samples required to split a node
min_samples_split = [4, 7, 10]
# Minimum number of samples required at each leaf node
min_samples_leaf = [4, 7, 10]

# Define the parameter grid for RandomizedSearchCV
param_grid = {
    "n_estimators": n_estimators,
    "max_depth": max_depth,
    "min_samples_split": min_samples_split,
    "min_samples_leaf": min_samples_leaf,
    "max_features": max_features,
    "split_criterion": [
        2
    ],  # 2 is MSE (source: https://stackoverflow.com/questions/77984286/cuml-randomforestclassifier-typeerror-an-integer-is-required)
}


# Initialize the Random Forest model
rf_model = RandomForestRegressor(split_criterion=2)  # split_criterion=2 for mse

time_start = time.time()
print("Random search starting...")


# Initialize the RandomizedSearchCV with 5-fold cross-validation
random_search = RandomizedSearchCV(
    estimator=rf_model,
    param_distributions=param_grid,
    n_iter=5,
    scoring="r2",
    cv=5,  # cv of 10 has the same accuracy but runs slower
    # verbose=2,
    # random_state=42,
    n_jobs=-1,
)

# Fit the RandomizedSearchCV to find the best hyperparameters
random_search.fit(X_train.to_numpy(), y_train.to_numpy())
time_end = time.time()
time_diff = time_end - time_start
print(f"Random search finished, elapsed {time_diff} seconds")

# Get the best parameters
best_params = random_search.best_params_
best_model = random_search.best_estimator_

time_start = time.time()
print("Predicting...")
# Predict on the test set using the best model
y_pred = best_model.predict(X_test)
time_end = time.time()
time_diff = time_end - time_start
print(f"Predicted#! Elapsed {time_diff} seconds")

# Calculate the Mean Squared Error
r2 = r2_score(y_test.to_numpy(), y_pred.to_numpy())

print(f"Best Parameters: {best_params}")
print(f"r2 score: {r2}")
# print(f"First 10 Predictions: {y_pred[:10]}")

good_known_params = {
    "split_criterion": 2,
    "n_estimators": 200,
    "min_samples_split": 4,
    "min_samples_leaf": 4,
    "max_features": "log2",
    "max_depth": 30,
}
# ^ not an interval

known_fast_model = RandomForestRegressor(**good_known_params)

time_start = time.time()
print("Known fit starting...")

# Fit the RandomizedSearchCV to find the best hyperparameters
known_fast_model.fit(X_train.to_numpy(), y_train.to_numpy())
time_end = time.time()
time_diff = time_end - time_start
print(f"Known fit finished, elapsed {time_diff} seconds")

time_start = time.time()
print("Predicting...")
# Predict on the test set using the best model
y_pred = known_fast_model.predict(X_test)
time_end = time.time()
time_diff = time_end - time_start
print(f"Predicted#! Elapsed {time_diff} seconds")

# Calculate the Mean Squared Error
r2 = r2_score(y_test.to_numpy(), y_pred.to_numpy())

print(f"r2 score: {r2}")
# print(f"First 10 Predictions: {y_pred[:10]}")

import rasterio

input_tif = "S2_Chautauqua_v2_modified.tif"

with rasterio.open(input_tif) as src:
    raster_data = src.read()
    profile = src.profile
    transform = src.transform
    crs = src.crs


# reshape raster data to 2D array for model prediction
n_bands, n_rows, n_cols = raster_data.shape
raster_data_2d = raster_data.reshape(n_bands, -1).T

# handle NaN values by replacing them with the mean of each band
nan_mask = np.isnan(raster_data_2d)
means = np.nanmean(raster_data_2d, axis=0)
raster_data_2d[nan_mask] = np.take(means, np.where(nan_mask)[1])


time_start = time.time()

print("Predicting...", time_start)

# perform the prediction
# NOTE::::: best_model (from a search upon interval) or known_fast_model (from GIVEN params)
# predictions = best_model.predict(raster_data_2d)
predictions = known_fast_model.predict(raster_data_2d)

time_end = time.time()
time_diff = time_end - time_start
print("Predicted at ", time_end)
print(f"Elapsed predicting time: {time_diff} seconds")

# reshape the predictions back to the original raster shape
predictions_raster = predictions.reshape(n_rows, n_cols)

# save the prediction result as a new raster file
output_tif = "S2_Chautauqua_v2_predictions.tif"

with rasterio.open(
    output_tif,
    "w",
    driver="GTiff",
    height=n_rows,
    width=n_cols,
    count=1,
    dtype=predictions_raster.dtype,
    crs=crs,
    transform=transform,
) as dst:
    dst.write(predictions_raster, 1)

# plot the result
plt.imshow(predictions_raster, cmap="viridis")
plt.colorbar()
plt.title("Predicted Values")
plt.show()

print(f"Predictions saved to {output_tif}")

# Cache of fast (randomly generated obviously) parameters
"""
Best Parameters: {'split_criterion': 2, 'n_estimators': 200, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 'log2', 'max_depth': 30}

"""
